{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"klueBERT_WiC.ipynb","provenance":[],"mount_file_id":"1Kt7UkPekJxs7QcBYBSZiph3JfQGEKPgc","authorship_tag":"ABX9TyM2GfC9M+X/o245wkjCb1eb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIi24KNNFGJM","executionInfo":{"status":"ok","timestamp":1632390994427,"user_tz":-540,"elapsed":335,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}},"outputId":"dffd0075-d249-4aa8-fa5f-58de7e7aabb8"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2WbE_ElgFN67","executionInfo":{"status":"ok","timestamp":1632390999724,"user_tz":-540,"elapsed":3258,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}},"outputId":"3f353841-43d4-41ef-ceb7-3b3df7f97438"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["No GPU available, using the CPU instead.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18qD3LjdFO-1","executionInfo":{"status":"ok","timestamp":1632391059857,"user_tz":-540,"elapsed":10646,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}},"outputId":"3070386c-8987-47a2-d5f9-de3c3a068494"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 6.6 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.0.12\n","  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 39.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 30.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 51.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"FYTYkC7zFP9k","executionInfo":{"status":"ok","timestamp":1632391087076,"user_tz":-540,"elapsed":850,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}},"outputId":"7232d18d-78db-4c8d-cbb7-60ef72eb185f"},"source":["import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","PATH = '/content/drive/MyDrive/gh/klue/DATA'\n","df = pd.read_csv(PATH + \"/WiC/NIKL_SKT_WiC_Train.tsv\", sep='\\t')\n","\n","# Report the number of sentences.\n","print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Display 10 random rows from the data.\n","df.sample(10)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training sentences: 7,748\n","\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Target</th>\n","      <th>SENTENCE1</th>\n","      <th>SENTENCE2</th>\n","      <th>ANSWER</th>\n","      <th>start_s1</th>\n","      <th>end_s1</th>\n","      <th>start_s2</th>\n","      <th>end_s2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6194</th>\n","      <td>6195</td>\n","      <td>벌어진</td>\n","      <td>특히 88년엔 조국 캐나다의 캘거리에서 벌어진 올림픽에서 미국의 브라이언 보이타노와...</td>\n","      <td>삼성에버랜드동물원에서는 물개쇼와 공작 등 조류 7종 500마리가 출연하는 새공연, ...</td>\n","      <td>True</td>\n","      <td>22</td>\n","      <td>25</td>\n","      <td>92</td>\n","      <td>95</td>\n","    </tr>\n","    <tr>\n","      <th>714</th>\n","      <td>715</td>\n","      <td>백미</td>\n","      <td>현미는 백미보다 더 좋은 건강식품이다.</td>\n","      <td>백미를 먹는 것은 달걀의 노른자위를 버리고 흰자위만 먹는 꼴입니다.</td>\n","      <td>True</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1375</th>\n","      <td>1376</td>\n","      <td>부인</td>\n","      <td>그의 부인은 공동묘지에 잠들어 있다.</td>\n","      <td>남편은 친절하며 부인은 인정이 많다.</td>\n","      <td>True</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>9</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>2022</th>\n","      <td>2023</td>\n","      <td>벽지</td>\n","      <td>떨어진 벽지를 밥풀로 붙였다.</td>\n","      <td>농촌의 산간벽지까지 전기가 들어간다.</td>\n","      <td>False</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>2811</th>\n","      <td>2812</td>\n","      <td>장부</td>\n","      <td>장부에 누락된 항목이 많다.</td>\n","      <td>회사에서 비밀 장부를 빼내다.</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>80</td>\n","      <td>소생</td>\n","      <td>병원 측에서는 환자가 소생이 될 기미가 없어 보이자 가족들에게 신병을 인도해 갈 것...</td>\n","      <td>정섭은 아버지가 돌아가신 후 큰어머니 소생의 이복형이 유산을 모두 차지하여 빈털터리...</td>\n","      <td>False</td>\n","      <td>12</td>\n","      <td>14</td>\n","      <td>21</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>5762</th>\n","      <td>5763</td>\n","      <td>쓰인</td>\n","      <td>충남 아산의 ‘연엽주’는 특이하게 연근과 솔잎이 재료로 쓰인다.</td>\n","      <td>영업시간은 밤 12시부터 오전 7시까지, 메뉴판에 쓰인 음식이라곤 돼지고기 된장국 ...</td>\n","      <td>False</td>\n","      <td>31</td>\n","      <td>33</td>\n","      <td>28</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>5635</th>\n","      <td>5636</td>\n","      <td>물</td>\n","      <td>재판이 길어져 연체 가산세가 불어나면서 황씨가 물어야 할 세금은 225억원에 달하게...</td>\n","      <td>유럽에도 외톨이 청년들이 위험한 미끼를 물면 헤어나질 못한다.</td>\n","      <td>False</td>\n","      <td>26</td>\n","      <td>27</td>\n","      <td>22</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>1497</th>\n","      <td>1498</td>\n","      <td>비상</td>\n","      <td>곧 남은 대원들에게 비상 명령이 내렸다</td>\n","      <td>중대장은 비상을 걸고 사단과 포대에 연락을 취하겠다고 말했다.</td>\n","      <td>True</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>5</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>7428</th>\n","      <td>7429</td>\n","      <td>들</td>\n","      <td>\"만약 이번 임기의 정부가 모두 예스맨(好好先生)만 있다면 우리는 정말 국민 앞에 ...</td>\n","      <td>굳은 얼굴로 심호흡을 하더니 단번에 역기를 허벅지까지 들어올렸다.</td>\n","      <td>True</td>\n","      <td>50</td>\n","      <td>51</td>\n","      <td>30</td>\n","      <td>31</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        ID Target  ... start_s2 end_s2\n","6194  6195    벌어진  ...       92     95\n","714    715     백미  ...        0      2\n","1375  1376     부인  ...        9     11\n","2022  2023     벽지  ...        6      8\n","2811  2812     장부  ...        8     10\n","79      80     소생  ...       21     23\n","5762  5763     쓰인  ...       28     30\n","5635  5636      물  ...       22     23\n","1497  1498     비상  ...        5      7\n","7428  7429      들  ...       30     31\n","\n","[10 rows x 9 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"clf3cUADFuud"},"source":["class WiCTask(SuperGLUE):\n","  def __init__(self, data_dir, tokenizer, **kwargs):\n","    super().__init__(tokenizer, **kwargs)\n","    self.data_dir = data_dir\n","\n","  def train_data(self, max_seq_len=512, dataset_size=None, epochs=1, mask_gen=None, debug=False, **kwargs):\n","    if debug:\n","      max_examples = 1000\n","    else:\n","      max_examples = None\n","\n","    train = self.load_data(os.path.join(self.data_dir, 'train.jsonl'), max_examples=max_examples)\n","    examples = ExampleSet(train)\n","    if dataset_size is None:\n","      dataset_size = len(examples)*epochs\n","    return DynamicDataset(examples, feature_fn = self.get_feature_fn(max_seq_len=max_seq_len, mask_gen=mask_gen), \\\n","dataset_size = dataset_size, shuffle=True, **kwargs)\n","\n","  def eval_data(self, max_seq_len=512, dataset_size=None, **kwargs):\n","    ds = [\n","        self._data('dev', 'val.jsonl', 'dev', max_examples=None),\n","        self._data('wordnet', 'train_weak.jsonl', 'dev'),\n","        ]\n","   \n","    for d in ds:\n","      if dataset_size is None:\n","        _size = len(d.data)\n","      d.data = DynamicDataset(d.data, feature_fn = self.get_feature_fn(max_seq_len=max_seq_len), dataset_size = _size, **kwargs)\n","    return ds\n","\n","  def test_data(self,max_seq_len=512, dataset_size = None, **kwargs):\n","    \"\"\"See base class.\"\"\"\n","    ds = [\n","        self._data('test', 'test.jsonl', 'test')\n","        ]\n","    \n","    for d in ds:\n","      if dataset_size is None:\n","        _size = len(d.data)\n","      d.data = DynamicDataset(d.data, feature_fn = self.get_feature_fn(max_seq_len=max_seq_len), dataset_size = _size, **kwargs)\n","    return ds\n","\n","  def _data(self, name, path, type_name = 'dev', ignore_metric=False, max_examples=None, shuffle=False):\n","    input_src = os.path.join(self.data_dir, path)\n","    assert os.path.exists(input_src), f\"{input_src} doesn't exists\"\n","    data = self.load_data(input_src, max_examples=max_examples, shuffle=shuffle)\n","    examples = ExampleSet(data)\n","    predict_fn = self.get_predict_fn()\n","    metrics_fn = self.get_metrics_fn()\n","    return EvalData(name, examples,\n","      metrics_fn = metrics_fn, predict_fn = predict_fn, ignore_metric=ignore_metric, critial_metrics=['accuracy'])\n","\n","  def get_metrics_fn(self):\n","    \"\"\"Calcuate metrics based on prediction results\"\"\"\n","    def binary_accuracy(logits, labels):\n","      pred = (logits>0).reshape(-1)\n","      match = pred==labels\n","      return np.sum(match)/len(labels)\n","\n","    def metrics_fn(logits, labels):\n","      return OrderedDict(\n","          #accuracy = binary_accuracy(logits, labels),\n","          accuracy = metric_accuracy(logits, labels),\n","          )\n","    return metrics_fn\n","\n","  def get_feature_fn(self, max_seq_len = 512, mask_gen = None):\n","    def _example_to_feature(example, rng=None, ext_params=None, **kwargs):\n","      return self.example_to_feature(self.tokenizer, example, max_seq_len = max_seq_len, \\\n","        rng = rng, mask_generator = mask_gen, ext_params = ext_params, **kwargs)\n","    return _example_to_feature\n","\n","  def get_labels(self):\n","    \"\"\"See base class.\"\"\"\n","    return ['false', 'true']\n","\n","  def load_data(self, path, is_train=False, max_seq_len=512, stride=256, max_examples=None, shuffle=False):\n","    with open(path) as fs:\n","      data = [json.loads(l) for l in fs]\n","    if shuffle:\n","      random.shuffle(data)\n","    if max_examples is not None and max_examples>0:\n","      data = data[:max_examples]\n","    examples=[]\n","    for d in data:\n","      s1 = d['sentence1']\n","      s2 = d['sentence2']\n","      word = d['word']\n","      label = str(d['label']).lower() if 'label' in d else None\n","      start1,end1 = d['start1'],d['end1']\n","      start2,end2 = d['start2'],d['end2']\n","      def tokenize_sentence(sent, start, end):\n","        words = self.tokenizer.split_to_words(sent)\n","        word_ends = np.cumsum([len(c) for c in words])\n","        word_tokens = [self.tokenizer.tokenize(w) for w in words]\n","        tokens = [t for wt in word_tokens for t in wt]\n","        word_token_ends = np.cumsum([len(t) for t in word_tokens])\n","        if start == end:\n","          end = sstart + 1\n","        ws = bisect(word_ends, start)\n","        we = bisect(word_ends, end)\n","        text = sent[start:end]\n","        if we==ws:\n","          we += 1\n","        span = [word_token_ends[ws-1] if ws>0 else 0, word_token_ends[we-1]]\n","        decoded = self.tokenizer.decode(tokens[span[0]:span[1]])\n","        decoded2 = self.tokenizer.decode(tokens[span[0]:span[1]-1]) if span[1]-span[0]>1 else ''\n","        while text in decoded2:\n","          span[1] -= 1\n","          _decoded2 = self.tokenizer.decode(tokens[span[0]:span[1]-1]) if span[1]-span[0]>1 else ''\n","          decoded2 = _decoded2\n","\n","        if text not in decoded or text in decoded2:\n","          logger.warning(f'[{text}]: [{decoded2}]: [{_decoded2}]')\n","          pdb.set_trace()\n","        return tokens, span\n","      s1_tokens, s1_span = tokenize_sentence(s1, start1, end1)\n","      s2_tokens, s2_span = tokenize_sentence(s2, start2, end2)\n","      example = ExampleInstance(segments=[s1_tokens, s2_tokens], label=label, spans=[s1_span, s2_span])\n","      examples.append(example)\n","\n","    def get_stats(l):\n","      return f'Max={max(l)}, min={min(l)}, avg={np.mean(l)}'\n","    s1_token_size = [len(e.segments[0]) for e in examples]\n","    s2_token_size = [len(e.segments[1]) for e in examples]\n","    span1 = [e.spans[0][1]-e.spans[0][0] for e in examples]\n","    total_size = [len(e.segments[0]) + len(e.segments[1]) for e in examples]\n","    logger.info(f'Sentence1 statistics: {get_stats(s1_token_size)}, long={len([t for t in s1_token_size if t > 500])}/{len(s1_token_size)}')\n","    logger.info(f'Sentence2 statistics: {get_stats(s2_token_size)}')\n","    logger.info(f'Word span statistics: {get_stats(span1)}')\n","    logger.info(f'Total statistics: {get_stats(total_size)}, long={len([t for t in total_size if t>500])}')\n","    return examples\n","\n","  def example_to_feature(self, tokenizer, example, max_seq_len=512, rng=None, mask_generator = None, ext_params=None, label_type='int', **kwargs):\n","    if not rng:\n","      rng = random\n","    max_num_tokens = max_seq_len - len(example.segments) - 1\n","    features = OrderedDict()\n","    tokens = ['[CLS]']\n","    type_ids = [0]\n","    segments = [example.segments[0], example.segments[1]]\n","    segments = _truncate_segments(segments, max_num_tokens, rng)\n","\n","    # Max word span 6\n","    max_word_span = 16\n","    words = example.spans\n","    word_indice = []\n","    for i,s in enumerate(segments):\n","      span = words[i]\n","      token_pos = list(range(span[0]+len(tokens), span[1] + len(tokens)))\n","      word_indice.append(token_pos + [0]*(max_word_span-len(token_pos)))\n","\n","      tokens.extend(s)\n","      tokens.append('[SEP]')\n","      type_ids.extend([i]*(len(s)+1))\n","\n","    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","    pos_ids = list(range(len(token_ids)))\n","    input_mask = [1]*len(token_ids)\n","    features['input_ids'] = token_ids\n","    features['type_ids'] = type_ids\n","    features['position_ids'] = pos_ids\n","    features['input_mask'] = input_mask\n","    padding_size = max(0, max_seq_len - len(token_ids))\n","    for f in features:\n","      features[f].extend([0]*padding_size)\n","\n","    features['word_spans'] = word_indice\n","\n","    for f in features:\n","      features[f] = torch.tensor(features[f], dtype=torch.int)\n","    if example.label is not None: # and example.label[0]>=0 and example.label[1]>=0:\n","      features['labels'] = torch.tensor(self.label2id(example.label), dtype=torch.int)\n","    return features\n","\n","  def get_model_class_fn(self):\n","    def partial_class(*wargs, **kwargs):\n","      return WiCModel.load_model(*wargs, **kwargs)\n","    return partial_class"],"execution_count":null,"outputs":[]}]}